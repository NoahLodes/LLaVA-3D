{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 11:52:03.362312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-19 11:52:04.342208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/lodes/miniconda3/envs/open3dsg/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/lodes/miniconda3/envs/open3dsg/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "sys.path.append( '/home/lodes/uni/3.semester/project/LLaVA-3D/open3dsg' )\n",
    "from transformers import InstructBlipVideoProcessor, InstructBlipVideoForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from const import CONF_PATH_R3SCAN_RAW, CONF_PATH_R3SCAN_PROCESSED\n",
    "from open_dataset import Open2D3DSGDataset\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scan(base_path, file_path):\n",
    "            return json.load(open(os.path.join(base_path, file_path)))[\"scans\"]\n",
    "\n",
    "def get_queries(data_dict):\n",
    "    obj_class_dict = [line.rstrip() for line in open(os.path.join(CONF_PATH_R3SCAN_RAW, \"classes.txt\"), \"r\").readlines()]\n",
    "    obj_count = data_dict['objects_count'].item()\n",
    "    rel_count = int(data_dict[\"predicate_count\"].item())\n",
    "    objects_gt = data_dict['objects_cat']\n",
    "    edges = data_dict['edges'][:rel_count]\n",
    "    object_edges = np.array(objects_gt[:obj_count][edges], dtype=np.int32)\n",
    "    object_edges = np.array(obj_class_dict)[object_edges]\n",
    "    object_edges[object_edges == 'socket'] = 'wall'\n",
    "    queries = [\n",
    "    f\"Describe the relationship between the {o[0]} and the {'other ' if o[0]==o[1] else ''}{o[1]}. Start the response with: the {o[0]}\" if o[0] != o[1]\n",
    "    else f\"Describe the relationship between the {o[0]} and the {'other ' if o[0]==o[1] else ''}{o[1]}. Start the response with: the {o[0]}\"\n",
    "    for o in object_edges]\n",
    "\n",
    "    return queries\n",
    "\n",
    "def is_black_image(image_array):\n",
    "    \"\"\"\n",
    "    Check if a given image array is completely black.\n",
    "    A completely black image has all pixel values equal to 0.\n",
    "    \"\"\"\n",
    "    return np.all(image_array == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83acea606d54199adaf7be887dcb5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scan_id = '754e884c-ea24-2175-8b34-cead19d4198d'\n",
    "D3SSG = load_scan(CONF_PATH_R3SCAN_RAW, \"relationships_train.json\")\n",
    "for r in D3SSG:\n",
    "    if r['scan'] == scan_id:\n",
    "        D3SSG = [r]\n",
    "dataset = Open2D3DSGDataset(\n",
    "    relationships_R3SCAN=D3SSG,\n",
    "    relationships_scannet=None,\n",
    "    openseg=False,\n",
    "    img_dim=224,\n",
    "    rel_img_dim=224,\n",
    "    top_k_frames=5,\n",
    "    scales=3,\n",
    "    mini=False,\n",
    "    load_features=None,\n",
    "    blip=True,\n",
    "    llava=False,\n",
    "    half=False,\n",
    "    max_objects=9,\n",
    "    max_rels=72\n",
    ")\n",
    "data_dict = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"device_map\": 0}\n",
    "kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        )\n",
    "cache_dir = '../models/BLIP'\n",
    "processor = InstructBlipVideoProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_images = data_dict['blip_images']\n",
    "image_shape = (960, 540, 3)\n",
    "black_image = np.zeros(image_shape, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = []\n",
    "for image_list in rel_images:\n",
    "    # Pad with black images if the list has fewer than 4 images\n",
    "    if len(image_list) < 4:\n",
    "        padded_list = image_list + [black_image] * (4 - len(image_list))\n",
    "    else:\n",
    "        padded_list = image_list\n",
    "    \n",
    "    # Select a random subset of 4 images\n",
    "    random_subset = [np.array(x) for x in random.sample(padded_list, 4)]\n",
    "    \n",
    "    # Check if all images in the subset are black\n",
    "    if all(is_black_image(image) for image in random_subset):\n",
    "        processed_images.append(np.empty((0,)))  # Append an empty array\n",
    "    else:\n",
    "        # Stack the subset and append it to the result\n",
    "        processed_images.append(np.stack(random_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type instructblip to instantiate a model of type instructblipvideo. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5be760a58fb4e36977362fba169ba7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = get_queries(data_dict)\n",
    "model = InstructBlipVideoForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\", cache_dir=cache_dir, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [23:26, 25.12s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_path = \"./blip_relationships/results.json\"\n",
    "assert len(queries) == len(processed_images)\n",
    "for idx, image_set in tqdm(enumerate(processed_images)):\n",
    "   prompt = queries[idx]\n",
    "   if np.all(image_set == 0):\n",
    "       results.append(\"No relationship\")\n",
    "   else:\n",
    "      inputs = processor(text=prompt, images=image_set, return_tensors=\"pt\").to(model.device)\n",
    "      outputs = model.generate(\n",
    "         **inputs,\n",
    "         do_sample=False,\n",
    "         num_beams=5,\n",
    "         max_length=256,\n",
    "         repetition_penalty=1.5,\n",
    "         length_penalty=1.0,\n",
    "      )\n",
    "      generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "      results.append(generated_text)\n",
    "   with open(results_path, \"w\") as json_file:\n",
    "      json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open3dsg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
