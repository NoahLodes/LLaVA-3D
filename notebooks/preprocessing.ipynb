{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPImageProcessor\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embodiedscan_frames(frames):\n",
    "    if not isinstance(frames, list):\n",
    "        frames = [frames]\n",
    "    if 'scannet' in frames[0] or '3rscan' in frames[0]:\n",
    "        images = []\n",
    "        depths = []\n",
    "        poses = []\n",
    "        if 'scannet' in frames[0]:\n",
    "            video = frames[0].split('/')[-4] + '/' + frames[0].split('/')[-2]\n",
    "        elif '3rscan' in frames[0]:\n",
    "            video = frames[0].split('/')[-4] + '/' + frames[0].split('/')[-3]\n",
    "        video_info = scene[video]\n",
    "        for frame in frames:\n",
    "            path = Path(frame)\n",
    "            frame_name = str(Path(*path.parts[-4:]))\n",
    "            pose = np.array(video_info[frame_name]['pose']) # 4x4 array\n",
    "            image = frame\n",
    "            if 'scannet' in frame:\n",
    "                depth = frame.replace('jpg', 'png')\n",
    "            elif '3rscan' in frame:\n",
    "                depth = frame.replace('color.jpg', 'depth.png')\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            # we need to ensure that the frame has valid pose\n",
    "            images.append(image)\n",
    "            depths.append(depth)\n",
    "            poses.append(pose)\n",
    "        depth_intrinsic_file = np.array(video_info['depth_intrinsic'])  # 4x4 array\n",
    "        intrinsic_file = np.array(video_info['intrinsic']) # 4x4 array\n",
    "        axis_align_matrix_file = np.array(video_info['axis_align_matrix'])  # 4x4 array\n",
    "        video_info = dict()\n",
    "        video_info['sample_image_files'] = images\n",
    "        video_info['sample_depth_image_files'] = depths\n",
    "        video_info['sample_pose_files'] = poses\n",
    "        video_info['depth_intrinsic_file'] = depth_intrinsic_file\n",
    "        video_info['intrinsic_file'] = intrinsic_file\n",
    "        video_info['axis_align_matrix_file'] = axis_align_matrix_file\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return video_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1716784140.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[46], line 58\u001b[0;36m\u001b[0m\n\u001b[0;31m    image = .image_processor.preprocess(images=image, do_rescale=do_rescale, do_normalize=do_normalize, return_tensors=return_tensors)['pixel_values'][0] # [3, H, W]\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def process_videos(videos, mode='random', device=None, text=None):\n",
    "    if isinstance(videos, str):\n",
    "        videos = [videos]\n",
    "    new_videos = []\n",
    "    for video in videos:\n",
    "        video = preprocess(video, return_tensors='pt', mode=mode, device=device, text=text)\n",
    "        new_videos.append(video)\n",
    "\n",
    "def preprocess( video: str, \n",
    "                return_tensors='pt', \n",
    "                mode='random', \n",
    "                device=None, \n",
    "                text=None,\n",
    "                do_rescale=True,\n",
    "                do_normalize=True,\n",
    "                do_depth_scale=True):\n",
    "        \"\"\"\n",
    "            video:  1. str video id / single video frame\n",
    "                    2. list  list of video frames\n",
    "        \"\"\"\n",
    "        video_info = extract_embodiedscan_frames(video)\n",
    "\n",
    "        dataset = video_info['dataset']\n",
    "        sample_frame_num = video_info['sample_frame_num']\n",
    "\n",
    "        depth_scale = 1000\n",
    "\n",
    "        images = []\n",
    "        depth_images = []\n",
    "        poses = []\n",
    "\n",
    "        if 'depth_intrinsic_file' in video_info:\n",
    "            depth_intrinsic = video_info['depth_intrinsic_file']\n",
    "            if not isinstance(depth_intrinsic, np.ndarray):\n",
    "                depth_intrinsic = np.loadtxt(depth_intrinsic)\n",
    "\n",
    "        intrinsic = video_info['intrinsic_file']  # (V, 4, 4) or (4, 4)\n",
    "        if not isinstance(intrinsic, np.ndarray):\n",
    "            intrinsic = np.loadtxt(intrinsic)\n",
    "\n",
    "        for id, image_file in enumerate(video_info['sample_image_files']):\n",
    "            image = Image.open(image_file).convert('RGB')\n",
    "            image_size = image.size\n",
    "            image = image_processor.preprocess(images=image, do_rescale=do_rescale, do_normalize=do_normalize, return_tensors=return_tensors)['pixel_values'][0] # [3, H, W]\n",
    "            depth_image = Image.open(video_info['sample_depth_image_files'][id])\n",
    "            depth_image_size = depth_image.size\n",
    "            depth_image, resize_shape = preprocess_depth_image(depth_image, do_depth_scale=do_depth_scale, depth_scale=depth_scale)\n",
    "            depth_image = torch.as_tensor(np.ascontiguousarray(depth_image)).float() # [H, W]\n",
    "            pose = video_info['sample_pose_files'][id]\n",
    "            if not isinstance(pose, np.ndarray):\n",
    "                pose = np.loadtxt(pose)\n",
    "            pose = torch.from_numpy(pose).float()  # [4, 4]\n",
    "            images.append(image)\n",
    "            depth_images.append(depth_image)\n",
    "            poses.append(pose)\n",
    "\n",
    "        #if dataset == 'scannet':\n",
    "            #intrinsic = preprocess_instrinsic(depth_intrinsic, depth_image_size, resize_shape)\n",
    "        #else:\n",
    "        intrinsic = preprocess_instrinsic(intrinsic, image_size, resize_shape)  # 3rscan / matterport\n",
    "\n",
    "        intrinsic = torch.from_numpy(intrinsic).float()\n",
    "\n",
    "        if intrinsic.dim() == 2:  # scannet/3rscan\n",
    "            intrinsic = intrinsic.unsqueeze(0).repeat(sample_frame_num, 1, 1)  # (V, 4, 4)\n",
    "\n",
    "        axis_align_matrix = video_info['axis_align_matrix_file']\n",
    "        if not isinstance(axis_align_matrix, np.ndarray):\n",
    "            axis_align_matrix = np.loadtxt(axis_align_matrix)\n",
    "\n",
    "        axis_align_matrix = torch.from_numpy(axis_align_matrix).float()\n",
    "        \n",
    "        # transform pose to axis_align pose\n",
    "        poses = [axis_align_matrix @ pose for pose in poses]\n",
    "\n",
    "        video_dict = dict()\n",
    "        video_dict['images'] = torch.stack(images)  # (V, 3, 336, 336)\n",
    "        video_dict['depth_images'] = torch.stack(depth_images)  # (V, 336,336)\n",
    "        video_dict['poses'] = torch.stack(poses)  # (V, 4, 4)\n",
    "        video_dict['intrinsic'] = intrinsic  # (V, 4, 4)\n",
    "\n",
    "        return video_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess() missing 1 required positional argument: 'video'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/3RScan/754e884c-ea24-2175-8b34-cead19d4198d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m videos_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m, in \u001b[0;36mprocess_videos\u001b[0;34m(videos, mode, device, text)\u001b[0m\n\u001b[1;32m      4\u001b[0m new_videos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos:\n\u001b[0;32m----> 6\u001b[0m     video \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     new_videos\u001b[38;5;241m.\u001b[39mappend(video)\n",
      "\u001b[0;31mTypeError\u001b[0m: preprocess() missing 1 required positional argument: 'video'"
     ]
    }
   ],
   "source": [
    "video_path = '../data/3RScan/754e884c-ea24-2175-8b34-cead19d4198d'\n",
    "videos_dict = process_videos(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id = '754e884c-ea24-2175-8b34-cead19d4198d'\n",
    "sub_key = f'3rscan/{scan_id}/sequence/frame-000000.color.jpg'\n",
    "with open('../data/embodiedscan_infos_full.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = None\n",
    "for key, value in data.items():\n",
    "    if key.startswith('3rscan'):\n",
    "        if sub_key in value:\n",
    "            scan = value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Scan from our dataset\n",
    "# 0.0123306 -0.999799 0.0158277 -0.00961019\n",
    "# -0.297868 0.0114376 0.954539 -0.00204422\n",
    "# -0.954527 -0.0164846 -0.297667 0.022526\n",
    "# 0 0 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pose': [[0.0123306, -0.999799, 0.0158277, -0.00961019],\n",
       "  [-0.297868, 0.0114376, 0.954539, -0.00204422],\n",
       "  [-0.954527, -0.0164846, -0.297667, 0.022526],\n",
       "  [0.0, 0.0, 0.0, 1.0]],\n",
       " 'depth': '3rscan/754e884c-ea24-2175-8b34-cead19d4198d/sequence/frame-000000.depth.pgm'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan[sub_key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open3dsg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
